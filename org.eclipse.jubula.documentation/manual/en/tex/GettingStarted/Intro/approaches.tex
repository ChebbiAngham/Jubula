% $Id: approaches.tex 7776 2009-01-30 17:08:26Z alexandra $
% Local Variables:
% ispell-check-comments: nil
% Local IspellDict: american
% End:
% --------------------------------------------------------
% User documentation
% copyright by BREDEX GmbH 2004
% --------------------------------------------------------

The methods and tools we previously evaluated were unsatisfactory 
for our testing needs. In the following section, we will explain a little more
about such methods, and why they did not meet our requirements.  

\subsubsection{Manual Tests}

Manual testing, although thorough, cannot keep up with the pace of development. It is impossible to carry out complete continuous integration and regression tests manually. 


\subsubsection{Programmed Tests}

Writing tests in some kind of scripting language is certainly powerful, but puts a strain on the resources of a test team, because programming staff (or automation experts) are needed to automate tests. The test code itself needs to be checked and maintained. The extra costs added by programming GUI tests can reach 50-60\% of the overall project cost. 


\subsubsection{Recorded Tests}

Another approach to automated testing is macro recording, that is, 
recording a user's actions for later playback.
This is a common approach, and seems the most appealing,
 but fails to meet the needs of large or long-term software projects 
for the following reasons:
\begin{itemize}
\item Test specification begins very late in the development cycle,
 as recording can only begin once the software is functional. 
\item Since only the user action is recorded, checkpoints for verification
of test results have to be inserted manually.
\item Recorded tests can only test parts of the application which already work. There is also the danger that the implementation of the application will be tested, instead of the requirements. 
\item Recorded scripts are often very large and not particularly 
well-structured. Making changes at a later point is therefore 
difficult and requires programming skills, which further increases costs. 
\item Maintainability is uneconomical, as there are two types of specification
to manage and update: the plain language version and the script which can be 
read by a machine. 
\item Code generated by recording generally doesn't conform to
 common software quality attributes such as reliability, stability, 
portability, maintainability, and usability.
\item An application under test (\gdaut) implemented in \bxname{Java} may not be fully supported, as
traditional recording techniques cannot deal with complex
 GUI elements such as those offered 
by e.g. \bxname{Swing} (table items, tabbed panes, trees, etc). Testing commands such as
  \begin{itemize}
    \item{''click into the pane with the heading 'title1'''}
    \item{''select the third tree element''}
    \item{''verify that the table item in row 4, column 5 is ...''}
  \end{itemize}
have proven difficult if not impossible to specify with the software that we evaluated.
\end{itemize}

\subsubsection{Conclusion}

Finally, we decided to write a tool ourselves that would overcome all
of the above shortcomings, while adding only minimal additional cost to 
the software project as a whole, and bringing maximum benefit. The result is \jb{}{}.  
